<div style="display: flex; justify-content: space-around; align-items: center;">
  <img src="assets/images/mlp_ts_in_ex.png" alt="Image 1" style="width: 100%; margin: 10px;">
</div>

## ğŸ“œ mlp-timeseries-interpolation-extrapolation
#### ğŸ“Œ Summary  
Implementation of a Multi-Layer Perceptron (MLP) using PyTorch for modeling, interpolation, and extrapolation of bivariate time series data `(x(t), y(t))` given scalar time input `t`.

#### ğŸ§  Overview  
This project leverages a deep learning-based MLP (Multi-Layer Perceptron) model to approximate noisy 2D time series data using Python and PyTorch. The model is trained to learn a mapping from scalar input time `t` to two-dimensional spatial coordinates `x` and `y`.  

This effectively enables:
- **Interpolation** â€” Estimating missing or unobserved values within the data range where 0<t<20
- **Extrapolation** â€” Predicting values beyond the existing time boundaries where 0<t<100

The original dataset (data.csv)[https://gist.github.com/tmramalho/51733432c88e2b834dbd70353849f887] is a noisy CSV file containing columns `t`, `x`, and `y`. Data preprocessing includes:
- Cleaning invalid characters
- Ensuring numerical consistency
- Plotting for visual inspection

The MLP model consists of several fully connected layers activated with ReLU, trained to minimize mean squared error between predicted and actual positions. The project was implemented and tested entirely in a Google Colab notebook environment.

**Model Architecture Summary - Interpolation**
- Input: Scalar value `t`
- Output: 2D vector `(x, y)`
- Layers:
  1. Dense Layer â†’ 64 units
  2. Dense Layer â†’ 128 units
  3. Dense Layer â†’ 256 units
  4. Output Layer â†’ 2 units `(x, y)`
- Activation: ReLU (in all hidden layers)

**Loss Function**
- Mean Squared Error (MSE)

**Optimization**
- Adam optimizer

**Model Architecture Summary - Extrapolation**
- Feature Extraction : Fourier Features
- Input: Scalar value `t`
- Output: 2D vector `(x, y)`
- Layers:
  1. Dense Layer â†’ 128 units
  2. Dense Layer â†’ 256 units
  3. Dense Layer â†’ 61 units
  4. Output Layer â†’ 2 units `(x, y)`
- Activation: ReLU (in all hidden layers)

**Loss Function**
- Mean Squared Error (MSE)

**Optimization**
- Adam optimizer

#### ğŸ¯ Use Cases
- Time series trajectory reconstruction
- Missing data interpolation
- Forecasting 2D trajectories (e.g., object motion)
- Learning noisy curve patterns
- Educational demonstration of function approximation with deep learning

#### ğŸŸ¢ Project Status
- Current Version: V1.0

#### ğŸ“‚ Repository Structure
```
mlp-timeseries-interpolation-extrapolation/
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ .gitignore                  
â”œâ”€â”€ assets/                      
â”‚   â””â”€â”€ images/
â””â”€â”€ notebooks/               
    â””â”€â”€ mlp_interpolation_extrapolation.ipynb
```

### âœ¨ Features
- âœ… Cleaned time series dataset
- âœ… MLP model for 2D prediction
- âœ… Visualization of real vs predicted curves
- âœ… Interpolation & extrapolation demonstration

ğŸ› ï¸ In progress:
- Performance comparison with other regressors

---

### ğŸš€ Getting Started

#### ğŸ“š Knowledge & Skills Required
- Python programming
- Basic ML/DL concepts
- Understanding of time series and regression

#### ğŸ’» Software Requirements
- Jupyter Notebook or Colab
- Python â‰¥ 3.8

#### ğŸ›¡ï¸ Tech Stack
- Language: Python
- Libraries: pandas, numpy, matplotlib, scikit-learn
- Deep Learning: PyTorch

#### ğŸ” Modules Breakdown

<b>ğŸ“¥ (1) Data Preprocessing:</b>
- Load and clean dataset (`data.csv`)
- Remove rows with invalid or missing entries
- Convert all values to numerical type
- Plot `x(t)` and `y(t)` for exploration

<b>ğŸ¤– (2) MLP Model - Interpoaltion:</b>
- Multi-layer fully connected neural network
- Input: scalar `t`, Output: vector `(x, y)`
- Layers: 1 â†’ 64 â†’ 128 â†’ 256 â†’ 2
- ReLU activations
- Output: 2D prediction of time-series position

<b>ğŸ“‰ (3) Loss & Optimization:</b>
- Loss Function: Mean Squared Error
- Optimizer: Adam

<b>ğŸ“Š Evaluation:</b>
- Plotting predicted vs true curves
- Visualization for both interpolation & extrapolation

---

#### âš™ï¸ Installation
```bash
git clone https://github.com/pointer2Alvee/mlp-timeseries-interpolation-extrapolation.git
cd mlp-timeseries-interpolation-extrapolation

# Recommended: Use virtual environment
pip install -r requirements.txt
```

##### ğŸ–‡ï¸ requirements.txt (core packages):
```
pandas
numpy
matplotlib
scikit-learn
torch
```

##### ğŸ’» Running the App Locally
1. Open the Jupyter notebook `mlp-timeseries.ipynb`
2. Run all cells sequentially
3. Visualize plots of predicted vs actual time series

---

### ğŸ“– Usage
- Open the project in **Google Colab** or **Jupyter Notebook**
- Adjust model layers or learning rate as needed
- Upload your own time series CSV (with `t`, `x`, `y` columns) for testing

---

### ğŸ§ª Sample Topics Implemented
- âœ… Deep MLP for regression
- âœ… Time series interpolation/extrapolation
- âœ… PyTorch model training
- âœ… Data visualization for time series

---

### ğŸ§­ Roadmap
- [x] Basic MLP model for 2D regression
- [x] Time series interpolation & extrapolation
- [x] Add Fourier-based feature transformation
- [ ] Compare with traditional regressors (SVR, GPR)

---

### ğŸ¤ Contributing
Contributions are welcomed!
1. Fork the repo  
2. Create a branch: `git checkout -b feature/YourFeature`  
3. Commit changes: `git commit -m 'Add some feature'`  
4. Push to branch: `git push origin feature/YourFeature`  
5. Open a Pull Request

---

### ğŸ“œ License
Distributed under the MIT License. See `LICENSE` for more information.

---

### ğŸ™ Acknowledgements
- PyTorch Team
- Kaggle Community
- Open-source contributors and visualization libraries

